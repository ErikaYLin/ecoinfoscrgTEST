---
title: "Living Shoreline Meta-Analysis Model Full Workflow"
author: "George Zaragoza, Erika Y. Lin, Christen H. Fleming"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This workflow demonstrates how the living shoreline suitability meta-analysis model is used, from initial data importing and cleaning, to mapped figures of shoreline suitability. This meta-analysis model, used to predict the suitability and probability of success of living shoreline restoration from the geospatial data, was built and based on the data and results of existing living shoreline suitability model (LSSM) studies.

# Setup

To work with spatial data for this meta-analysis model, the `sf` R package is needed. Other packages may also be helpful to load for data handling and visualization.

```{r packages, message = FALSE}

# If the packages are not installed, run the following:
# install.packages("sf")
# install.packages("dplyr")
# install.packages("ggplot2")

# Load packages
sf::sf_use_s2(FALSE)  # for working with planar data
library(sf)  # spatial data
library(dplyr)  # data wrangling and handling
library(ggplot2)  # data visualization

# Source script with data handling functions
source("R/standardize_impute.R")

```

# Import & Clean Shape (.shp) File Data

The `cleaningWrangling()` function automates the process of cleaning and combining data. The output is a list that includes the combined response variables and study names (`state`), the corresponding table of predictor variables (`predictors`), and vectors of all numeric variables (`numerical_vars`), all categorical variables (`categorical_vars`), and all binary variables (`binary_vars`). The arguments for `wranglingCleaning()` are `data` and `response`, which should be formatted as follows:

* `data` should be a named list of shape (shp) files, 1 file per study
* `response` is a vector of shoreline suitability response variable names, 1 per study

Note that the working directory for this workflow is the "SCRG/ecoinfoscrg" folder from https://github.com/ecoinformatic/SCRG. The specific code for certain scripts can be found in the "additions" branch of the repository.

```{r cleaning}

## Import shape file data
# Choctawatchee Bay data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
# choc <- st_read("./Data/choctawatchee_bay/choctawatchee_bay_lssm_POINTS_0.001deg.shp")
choc <- st_read("~/R/SCRG/Data/choctawatchee_bay/choctawatchee_bay_lssm_POINTS_0.001deg.shp")

# Pensacola Bay data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
# pens <- st_read("./Data/santa_rosa_bay/Santa_Rosa_Bay_Living_Shoreline_POINTS_0.001deg.shp")
pens <- st_read("~/R/SCRG/Data/santa_rosa_bay/Santa_Rosa_Bay_Living_Shoreline_POINTS_0.001deg.shp")

# Tampa Bay data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
# tampa <- st_read("./Data/tampa_bay/Tampa_Bay_Living_Shoreline_Suitability_Model_Results_POINTS_0.001deg.shp")
tampa <- st_read("~/R/SCRG/Data/tampa_bay/Tampa_Bay_Living_Shoreline_Suitability_Model_Results_POINTS_0.001deg.shp")

# Indian River Lagoon data (transformed 0.001deg.shp from WGS 84 to 6346 17N )
# IRL <- st_read("./Data/indian_river_lagoon/UCF_livingshorelinemodels_MosquitoNorthIRL_111m.shp")
IRL <- st_read("~/R/SCRG/Data/indian_river_lagoon/UCF_livingshorelinemodels_MosquitoNorthIRL_111m.shp")

## Note that the response variable for IRL does not follow the same evaluation
## convention as the other LSSMs
# This response will need to be converted separately to levels 1-3:
IRL <- IRL %>%
  mutate(Priority = as.numeric(case_when(
    Priority %in% c(0, 1) ~ 1,
    Priority %in% c(2, 3) ~ 2,
    Priority %in% c(4, 5) ~ 3,
    TRUE ~ Priority  # keeps other values unchanged
  )))

# Rename "Erosion" variable in IRL to avoid duplicate variable names
colnames(IRL)[10] <- "Erosion_1"

# Combine data into a named list
data <- list(choc = choc, pens = pens, tampa = tampa, IRL = IRL)

# Create vector of response variables in the order of the studies
response <- c("SMMv5Class", "SMMv5Class", "BMPallSMM", "Priority")


# Clean data using wranglingCleaning()
data <- wranglingCleaning(data = data, response = response)

# After wranglingCleaning.R
pred <- data$predictors  # store predictor data separately for easy access
state <- data$state  # store response data separately for easy access
# numerical_vars <- data$numerical_vars  # store variable names in separate vectors
categorical_vars <- data$categorical_vars
binary_vars <- data$binary_vars

# Additional manual spell check
for (var in c(categorical_vars, binary_vars)) {
  print(unique(pred[[var]]))  # check for additional inconsistencies in data entry
}

# Fix unique cases of misspellings
pred$Adj_H2[44059] <- "V"  # original spelling had an extra comma

# For binary variables not entered as Yes/No or Y/N or 1/0, change to 1/0
pred$canal <- gsub("Canal", "1", pred$canal)

# Update predictors in data object for next steps
data$predictors <- pred

```

The data is now clean and all studies have been merged into a single predictor dataset. We will now impute variables with missing values and standardize numerical variables.

# Impute Missing Data & Standardize 

Datasets may have missing values for certain predictor variables. For numerical and binary variables, we can use imputation techniques to fill these missing values at the site-wide scale and at the state-wide scale. Available techniques include using the median of the existing values for each variable or using the mean. Alternatively, working with spatial data offers the opportunity to use Kriging, a geostatistical method for spatial interpolation, because variables may be spatially autocorrelated. Kriging is only available at the site-wide scale, as it relies on using spatial autocorrelation for interpolation. The option to impute, and which method to use for imputation, are up to the user. However, we recommend Kriging if possible to better inform the model. 

Here, we have chosen to use auto-Kriging to estimate missing values for site-wide numeric predictors where possible, and the remaining numeric predictors will be imputed with the state-wide mean values. We will also standardize our data to be centered on 0 with a standard deviation of 1, so that values are comparable across sites and variables. The mean and standard deviation used for standardization for each variable is saved to ensure that the standardization process is consistent.

```{r standard, eval = FALSE}

# Impute and standardize the data
data2 <- standardize(data = data, site.method = "krige", state.method = "meanImpute")
# data <- standardize(data = data, site.method = "krige", state.method = "meanImpute")
# save(data2, file = "data/data2_standardize_impute.rda")

```

```{r standard2}

load(file = "data/data2_standardize_impute.rda")

```

# Model Selection

<!-- Give options for which model to use -->
Model selection is done by the `BUPD.R` and BUPD_nonparallel.R` scripts. These scripts are parallelized (for faster computation) and non-parallel, respectively. We recommend using the non-parallel script on non-Unix-based operating systems (i.e. Windows systems), as we do here. We use the ordinal probit regression for this data, to model our ordered, categorical response for shoreline suitability (categories 1, 2, 3, with 3 being the best and 1 being the worst) as a function of the other combined predictors from the LSSM studies. AIC-based model selection is performed in a step-wise manner using the "build-up, pair-down" (BUPD) method, that evaluates the performance of models built by running through and adding predictors one at a time and then reversing the process by removing a predictor at a time, starting from the best model. The probit link function allows us to transform the response from integers of 1, 2, and 3, to a probability ranging 0-1.

Model selection via BUPD is done for each study separately, before combining the results into a meta-analysis model that will enable comparable predictions of living shoreline suitability along the entire Florida coastline. The process saves the final selected model (lowest AIC) for each site, as well as the model formula and odds ratios. **Here, we demonstrate the model selection process for each of the four studies, using the parallelized `BUPD.R` script. Please note that the model selection process will take longer to run in the unparallelized form (`BUPD_nonparallel.R`).**

```{r BUPD, eval = FALSE}

# Retrieve response data
resp <- data2$state
resp$Response <- as.integer(resp$Response)  # must be an integer sequence from 1:K

# Define the response variable
response_var <- "Response"

# Retrieve predictor data
pred <- data2$predictors
pred <- pred %>%
  mutate(across(c(OBJECTID, ID, bmpCountv5, n, distance, X, Y), as.character))
## keep non predictor data as characters to avoid being selected as predictors

# Filter by study
resp_choc <- resp %>% filter(study == "choc")
resp_pens <- resp %>% filter(study == "pens")
resp_tampa <- resp %>% filter(study == "tampa")
resp_IRL <- resp %>% filter(study == "IRL")

pred_choc <- pred %>% filter(study == "choc")
pred_pens <- pred %>% filter(study == "pens")
pred_tampa <- pred %>% filter(study == "tampa")
pred_IRL <- pred %>% filter(study == "IRL")

# List all predictors (column names of known predictors)
numeric_pred <- pred %>%
  select_if(is.numeric)
factor_pred <- pred %>%
  select_if(is.factor)
predictors_full <- colnames(cbind(factor_pred, numeric_pred))

## Choctawatchee
# combine response and pred
data <- cbind(resp_choc, pred_choc)

# Specify a short name of the model
name <- "choc_ordinal_bounded"

# Define the response variable
response_var <- "Response"

study <- data.frame(study = resp$study)
input <- cbind(resp_choc, pred_choc)
input$SMMv5Def <- NULL
input$study <- as.factor(input$study)

# Run build-up/pair-down R scripts
start_time <- Sys.time()
source("inst/scripts/BUPD.R")
end_time <- Sys.time()

## Pensacola
# combine response and pred
data <- cbind(resp_pens, pred_pens)  

# Specify a short name of the model
name <- "pens_ordinal_bounded"

# Define the response variable
response_var <- "Response"

study <- data.frame(study = resp$study)
input <- cbind(resp_pens, pred_pens)
input$SMMv5Def <- NULL
input$study <- as.factor(input$study)

# Run build-up/pair-down R scripts
start_time <- Sys.time()
source("inst/scripts/BUPD.R")
end_time <- Sys.time()

## Tampa
# combine response and pred
data <- cbind(resp_tampa, pred_tampa)  

# Specify a short name of the model
name <- "tampa_ordinal_bounded"

# Define the response variable
response_var <- "Response"

study <- data.frame(study = resp$study)
input <- cbind(resp_tampa, pred_tampa)
input$SMMv5Def <- NULL
input$study <- as.factor(input$study)

# Run build-up/pair-down R scripts
start_time <- Sys.time()
source("inst/scripts/BUPD.R")
end_time <- Sys.time()

## IRL
# combine response and pred
data <- cbind(resp_IRL, pred_IRL)  

# Specify a short name of the model
name <- "IRL_ordinal_bounded"

# Define the response variable
response_var <- "Response"

study <- data.frame(study = resp$study)
input <- cbind(resp_IRL, pred_IRL)
input$SMMv5Def <- NULL
input$study <- as.factor(input$study)

# Run build-up/pair-down R scripts
start_time <- Sys.time()
source("inst/scripts/BUPD.R")
end_time <- Sys.time()

```

# Retrieving the Beta Coefficients

As noted previously, the following code is based on the previous ordinal probit regression model with unbounded thresholds (cumulative link model with probit link using the `ordinal::clm()` function). This model will technically result in incomparable responses across studies. The "additions" branch of the "ecoinfoscrg" Github repository contains the most up-to-date versions of the following scripts that are sourced, meaning they work using the unbounded `clm` ordinal probit model. Once the model selection for the corrected version of the model is complete, we will use those results for the meta-analytic regression. 

```{r betas, eval = FALSE}

## Previous ordinal probit model structure (flexible thresholds)
# ordinal::clm(formula, data = input, Hess = TRUE, method = "optim", link = "probit")

## For each of the models above
# Recreate coefficients table in model summary
vcov.clm <- final_model$vcov  # extract VAR-COV matrix
# Build matrix to summarize model coefficients
coefs <- matrix(NA, length(final_model$coefficients), 4,
                dimnames = list(names(final_model$coefficients),
                                c("Estimate", "Std. Error", "z value", "Pr(>|z|)")))
coefs[, 1] <- final_model$coefficients
alias.clm <- unlist(final_model$aliased)
coefs[!alias.clm, 2] <- sd <- sqrt(diag(vcov.clm))  # standard error
coefs[!alias.clm, 3] <- coefs[!alias.clm, 1]/coefs[!alias.clm, 2]  # Z-value
coefs[!alias.clm, 4] <- 2 * pnorm(abs(coefs[!alias.clm, 3]),
                              lower.tail=FALSE)

```

```{r betas2, eval = FALSE}

# Retrieve beta coefficients and covariance matrices for the betas
source("R/getBetas.R")  # averages beta estimates/SEs and combines the studies
source("R/varCov.R")  # retrieves the covariance matrix
## matrix cleaning and adjustments is done via `clean_matrix.R` script

```

# Meta-Analysis Model

```{r metafor, eval = FALSE}

# Define predictor columns
predictor_columns <- colnames(combined_betas_only)

# Create the formula string dynamically
formula_string <- paste("~", paste(predictor_columns, collapse = " + "))
formula <- as.formula(formula_string)

# Prepare the beta estimates
betas <- combined_betas_only

## META-ANALYTIC REGRESSION (using variances, not covariances)

# Flatten beta coefficients
betas_vector <- as.vector(t(combined_betas_only))

# Get study vector corresponding to each beta (required for study random effect)
study_labels <- c("choc", "pens", "tampa", "IRL") 
## will need to change labels as more data is available
study_vector <- rep(study_labels, each = ncol(combined_betas_only))

# Specify input data
beta = betas_vector
study = factor(study_vector)
predictor = rep(colnames(combined_betas_only), times = length(study_labels))
variance <- as.vector(t(VAR))

# FUNCTION FOR CALLING rma.mv (`meta_regression` will run with default options )
meta_regression <- function(
  beta = NULL,
  variance = NULL,
  predictor = NULL,
  study = NULL,
  method = "REML",
  struct = "UN",
  verbose = FALSE
) {
  if (is.null(beta)) beta <- get("beta", envir = .GlobalEnv) # default option
  if (is.null(variance)) variance <- get("variance", envir = .GlobalEnv) # default option
  if (is.null(predictor)) predictor <- get("predictor", envir = .GlobalEnv) # default option
  if (is.null(study)) study <- get("study", envir = .GlobalEnv) # default option

  result <- rma.mv(
    yi = beta,          # Vector of all beta coefficients
    V = variance,       # Vector of sampling variances or variance-covariance matrix
    method = method,    # Method for the meta-analysis
    mods = ~ predictor, # Including predictors as fixed effects
    verbose = verbose   # Verbosity of the output
  )

  return(result)
}

unscaled.meta <- meta_regression()

# saveRDS(unscaled.meta, file = "../output/unscaled_meta2.rds")


# SCALE BETAS TO REFERENCE STUDY

# Tampa as reference
scale_reference <- 3

# Function to scale betas
# scale_betas function scales each beta coef by theta/GM(theta)
# covs are scaled by theta%*%t(theta)/(GM(theta)^2), 
# but due to r code structure, we use thetas%*%thetas/(GM(theta)^2)
# thetas is a vector of scaling parameters
# keep is a vector of column indices to hold constant (not scale), if any
scale.betas <- function (thetas, adjust = TRUE) {
  results <- list()
  scaled_betas <- combined_betas_only
  scaled_cov <- cov_matrix
  scaled_var <- VAR
  gm_thetas <- prod(thetas)^(1/length(thetas))
  scalars.normalized <- thetas/gm_thetas
  for (i in 1:length(thetas)){
    scaled_betas[i, ] <- scaled_betas[i, ]*scalars.normalized[i]
  }
  # normalize it and then square
  # need to fix in order to accommodate for per-site cov matrices
  # this can be easily implemented by taking each normalized scalar (theta/gm_thetas)
  # and then squaring them to obtain the cov matrix scalar for each site
  cov.scalar <- scalars.normalized^2

  if (adjust) {

    scaled_cov_new <- list()
    scaled_cov_adjusted <- list()
    for (i in 1:length(scaled_cov)) {
      cov.names <- dimnames(scaled_cov[[i]])  # save dimnames

      # Scale covariance matrices
      scaled_cov_new[[i]] <- cov.scalar[i]*scaled_cov[[i]]
      # Clean each covariance matrix
      scaled_cov_adjusted[[i]] <- clean_matrix(scaled_cov_new[[i]])
    }

    # Grab variances from each matrix
    scaled_var_adjusted <- scaled_var  # simulate structure of combined betas dataframe
    for (i in 1:nrow(scaled_var_adjusted)) {
      for (j in 1:ncol(scaled_var_adjusted)) {

        # Add variances to corresponding predictor and study
        # VAR[i,j] <- diag(cov_matrix[[i]])[which(names(diag(cov_matrix[[i]])) == colnames(VAR)[j])]
        scaled_var_adjusted[i,j] <-
          diag(scaled_cov_adjusted[[i]])[which(
            names(diag(scaled_cov_adjusted[[i]])) == colnames(scaled_var_adjusted)[j])]
      }
    }

    results[[1]] <- scaled_betas
    results[[2]] <- scaled_cov_adjusted
    results[[3]] <- scaled_var_adjusted

  } else {

    # No adjustments
    scaled_cov_new <- list()
    for (i in 1:length(scaled_cov)) {
      cov.names <- dimnames(scaled_cov[[i]])  # save dimnames

      # Scale covariance matrices
      scaled_cov_new[[i]] <- cov.scalar[i]*scaled_cov[[i]]
    }

    # VARIANCES
    scaled_var_new <- scaled_var
    for (i in 1:nrow(scaled_var)) {

      # Scale variances
      scaled_var_new[i,] <- cov.scalar[i]*scaled_var[i,]
    }

    results[[1]] <- scaled_betas
    results[[2]] <- scaled_cov_new
    results[[3]] <- scaled_var_new
  }
  return (results)
}


# Function to calculate log-likelihood of meta-analytic regression 
# with given scaling parameters for beta
# Takes in nr_thetas, meaning non-reference thetas: 
## a vector of all thetas in order of study site, excluding the reference site
meta.ll <- function (log.thetas, adjust = FALSE) {
  nr_thetas <- exp(log.thetas)  # exponentiate after optimizing log.thetas
  thetas <- c(nr_thetas[0:(scale_reference-1)], 1, 
              nr_thetas[scale_reference:length(nr_thetas)])
  scaled_betas <- scale.betas(thetas, adjust = adjust)
  # -----------------
  # Formatting betas to a format the meta_regression can take
  # Not sure if there is a more efficient way
  betas_vec <- as.vector(t(scaled_betas[[1]]))
  # -----------------
  # Retrieve variances
  covs <- scaled_betas[[2]]  # extract scaled covariance matrices
  scaled_VAR <- scaled_betas[[3]]  # simulate structure of combined betas dataframe

  model <- meta_regression(beta = betas_vec, variance = as.vector(t(scaled_VAR)))
  ll <- model[["fit.stats"]]["ll", "ML"]
  return(ll)
}


# Estimate scaling parameters theta by maximizing meta analytic regression likelihood 
# (log likelihood is maximized here)

# optim.results <- optim(c(1,1,1), fn=meta.ll, method="L-BFGS-B", control = list(fnscale = -1))
optim.results <- optim(c(0,0,0), fn=meta.ll, control=list(fnscale=-1))

# thetas.maxll <- readRDS("../output/scaling_thetas2.rds")

# -----------------
# Fitting model with new thetas
exp.thetas <- exp(thetas.maxll[["par"]])
thetas <- c(exp.thetas[0:(scale_reference-1)], 1, 
            exp.thetas[scale_reference:length(exp.thetas)])
thetas.normalized <- thetas/(prod(thetas)^(1/length(thetas)))  # for checking thetas
scaled_betas <- scale.betas(thetas)
betas_vec <- as.vector(t(scaled_betas[[1]]))
covs <- scaled_betas[[2]]
scaled_VAR <- scaled_betas[[3]]

# Scaled meta-analytic regression model
scaled.meta <- meta_regression(beta = betas_vec, variance = as.vector(t(scaled_VAR)))

# Double-check thetas
thetas  # 1.547785e+02 3.734221e-06 1.000000e+00 2.502109e+02
thetas.normalized  # 2.509900e+02 6.055441e-06 1.621607e+00 4.057439e+02

```

Unfortunately, there seems to be an issue with scaling the betas, thus results were generates using the unscaled meta-analysis model.

# Predictions

```{r predictions, eval = FALSE}

# Set base map for plots
basemaps::set_defaults(map_service = "esri", map_type = "world_imagery")

# Import meta-analytic regression model
# meta_analysis <- readRDS("../output/scaled_meta2.rds")  # scaled model
meta_analysis <- readRDS("../output/unscaled_meta2.rds")  # unscaled model

# Retrieve studies for predictions
pred <- readRDS("data/predictors_kriged_standardized.rds")
colnames(pred)[41] <- "predicted_OLD"  # rename to compare to new predictions
predictor_data <- as.data.frame(pred)
predictors <- meta_analysis$data$predictor[1:248]  # retrieve predictors

# Remove non-predictor columns
predictor_data <- predictor_data %>%
  select(all_of(predictors)) %>%
  select(order(colnames(.)))

# Reformat predictor columns
predictor_data <- predictor_data %>%
  # Convert predictors to character first (needed for factorized columns)
  mutate(across(all_of(predictors), as.character)) %>% 
  mutate(across(all_of(predictors), as.numeric)) %>% # convert all predictors to numeric
  mutate(across(all_of(predictors), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))  
## Sub NAs for statewide means
## NAs in the data can result in predicted values to be NA

# Extract coefficients and intercept from final meta-analysis model
betas <- meta_analysis$beta  

# Remove "predictor" from predictor names
rownames(betas) <- gsub("predictor", "", rownames(betas))  # match predictor names
setdiff(colnames(predictor_data), rownames(betas))  # which predictor used as the intercept?
rownames(betas)[1] <- setdiff(colnames(predictor_data), rownames(betas))  # replace w/ pred name
betas <- as.vector(betas)

# Multiply predictors by the meta-analysis model betas
expected_outcome <- as.matrix(predictor_data) %*% betas

# Inverse probit for predicted values
predicted <- pnorm(expected_outcome)

# Add predictions to predictors data
pred$predicted <- predicted

# Load in original predictions and data for each site
choc <- st_transform(st_read("../Data/choctawatchee_bay/choctawatchee_bay_lssm_POINTS_0.001deg.shp"), crs = 6346) # Response:
pens <- st_transform(st_read("../Data/santa_rosa_bay/Santa_Rosa_Bay_Living_Shoreline_POINTS_0.001deg.shp"), crs = 6346) # Response:
tampa <- st_transform(st_read("../Data/tampa_bay/Tampa_Bay_Living_Shoreline_Suitability_Model_Results_POINTS_0.001deg.shp"), crs = 6346) # Response:
IRL <- st_transform(st_read("../Data/indian_river_lagoon/UCF_livingshorelinemodels_MosquitoNorthIRL_111m.shp"), crs = 6346) # Response:

# Full data to fix transformed geometry
choc_full <- st_transform(st_read("data/choctawatchee_bay/choctawatchee_bay_lssm_POINTS_0.001deg.shp"))
IRL_full <- st_transform(st_read("../output/Final_Shapefile_all_data/Indian River Lagoon/IRL_predicted.shp"))
pens_full <- st_transform(st_read("../output/Final_Shapefile_all_data/Pensacola Bay/pens_predicted.shp"))
tampa_full <- st_transform(st_read("../output/Final_Shapefile_all_data/Tampa Bay/tampa_predicted.shp"))

coo <- c("OBJECTID", "ID", "geometry", "feature_x", "feature_y", "nearest_x", 
         "nearest_y", "shape__len", "Shape__Len", "distance", "distance_2", "n", 
         "x", "y", "X", "Y")
pred_dat <- as.data.frame(pred)

# Add predictions to each site

## Choctawatchee
choc_available_columns <- coo[coo %in% colnames(choc)]
choc_coo <- choc %>% select(all_of(choc_available_columns))
choc_pred <- pred_dat[pred_dat$study == "choc",]
choc_pred$predicted <- pred_dat[pred_dat$study == "choc", "predicted"]
choc_cols <- intersect(colnames(choc_pred), colnames(choc_coo))
choc_pred <- choc_pred %>%
  select(!all_of(choc_cols))

choc_predicted <- cbind(choc_coo, choc_pred)
choc_test <- choc_predicted
st_geometry(choc_test) <- choc_full$geometry
choc_test <- choc_test %>%
  select(all_of(c("feature_x", "feature_y", "predicted")))  # automatically selects geometry
colnames(choc_test)[c(1,2)] <- c("X", "Y")

plot_choc <- st_transform(choc_test, crs = 3857)   
choc_bb <- st_bbox(c(xmin = -9644651-15000,
                     ymin = 3551143-15000,
                     xmax = -9584357+15000,
                     ymax = 3570713+15000), crs = 3857)
choc_coords <- as.data.frame(st_coordinates(plot_choc))
plot_choc2 <- cbind(plot_choc[,-c(1,2)], choc_coords)

# Choctawatchee Bay predictions map
choc_map <- basemaps::basemap_ggplot(ext = choc_bb) +  # ESRI World Imagery basemap
  geom_sf(data = plot_choc2, aes(x = X, y = Y, color = predicted), size = 1) +
  labs(x = "Latitude", y = "Longitude", title = "Choctawatchee Bay") +
  scale_color_gradient2(name = "Predicted", midpoint = 0.08,
                        low = "#B31B1B", mid = "#FCF75E", high = "#0BDA51") +
  theme(plot.background = element_blank(),
        panel.background = element_blank(),
        panel.grid = element_blank(),
        legend.background = element_blank())


## Pensacola Bay
pens_available_columns <- coo[coo %in% colnames(pens)]
pens_coo <- pens %>% select(all_of(pens_available_columns))
pens_pred <- pred_dat[pred_dat$study == "pens",]
pens_pred$predicted <- pred_dat[pred_dat$study == "pens", "predicted"]
pens_cols <- intersect(colnames(pens_pred), colnames(pens_coo))
pens_pred <- pens_pred %>%
  select(!all_of(pens_cols))

pens_predicted <- cbind(pens_coo, pens_pred)
pens_test <- pens_predicted
st_geometry(pens_test) <- pens_full$geometry
pens_test <- pens_test %>%
  select(all_of(c("feature_x", "feature_y", "predicted")))  # automatically selects geometry
colnames(pens_test)[c(1,2)] <- c("X", "Y")

plot_pens <- st_transform(pens_test, crs = 3857)
pens_bb <- st_bbox(c(xmin = -9732366-13000,
                     ymin = 3542349-13000,
                     xmax = -9659068+13000,
                     ymax = 3589286+13000), crs = 3857)
pens_coords <- as.data.frame(st_coordinates(plot_pens))
plot_pens2 <- cbind(plot_pens[,-c(1,2)], pens_coords)

# Pensacola Bay predictions map
pens_map <- basemaps::basemap_ggplot(ext = pens_bb) +
  geom_sf(data = plot_pens2, aes(x = X, y = Y, color = predicted), size = 1) +
  labs(x = "Latitude", y = "Longitude", title = "Pensacola Bay") +
  scale_color_gradient2(name = "Predicted", midpoint = 0.5,
                        low = "#B31B1B", mid = "#FCF75E", high = "#0BDA51") +
  theme(plot.background = element_blank(),
        panel.background = element_blank(),
        panel.grid = element_blank(),
        legend.background = element_blank())


## Tampa Bay
tampa_available_columns <- coo[coo %in% colnames(tampa)]
tampa_coo <- tampa %>% select(all_of(tampa_available_columns))
tampa_pred <- pred_dat[pred_dat$study == "tampa",]
tampa_pred$predicted <- pred_dat[pred_dat$study == "tampa", "predicted"]
tampa_cols <- intersect(colnames(tampa_pred), colnames(tampa_coo))
tampa_pred <- tampa_pred %>%
  select(!all_of(tampa_cols))

tampa_predicted <- cbind(tampa_coo, tampa_pred)
tampa_test <- tampa_predicted
st_geometry(tampa_test) <- tampa_full$geometry
tampa_test <- tampa_test %>%
  select(all_of(c("feature_x", "feature_y", "predicted")))  # automatically selects geometry
colnames(tampa_test)[c(1,2)] <- c("X", "Y")

plot_tampa <- st_transform(tampa_test, crs = 3857)   
tampa_bb <- st_bbox(c(xmin = -9222837-15000,
                      ymin = 3178783-15000,
                      xmax = -9161061+15000,
                      ymax = 3253985+15000), crs = 3857)
tampa_coords <- as.data.frame(st_coordinates(plot_tampa))
plot_tampa2 <- cbind(plot_tampa[,-c(1,2)], tampa_coords)

# Tampa Bay predictions map
tampa_map <- basemaps::basemap_ggplot(ext = tampa_bb) +
  geom_sf(data = plot_tampa2, aes(x = X, y = Y, color = predicted), size = 1) +
  labs(x = "Latitude", y = "Longitude", title = "Tampa Bay") +
  scale_color_gradient2(name = "Predicted", midpoint = 0.00013,
                        low = "#B31B1B", mid = "#FCF75E", high = "#0BDA51") +
  theme(plot.background = element_blank(),
        panel.background = element_blank(),
        panel.grid = element_blank(),
        legend.background = element_blank())


## IRL
IRL_available_columns <- coo[coo %in% colnames(IRL)]
IRL_coo <- IRL %>% select(all_of(IRL_available_columns))
IRL_pred <- pred_dat[pred_dat$study == "IRL",]
IRL_pred$predicted <- pred_dat[pred_dat$study == "IRL", "predicted"]
IRL_cols <- intersect(colnames(IRL_pred), colnames(IRL_coo))
IRL_pred <- IRL_pred %>%
  select(!all_of(IRL_cols))

IRL_predicted <- cbind(IRL_coo, IRL_pred)
IRL_test <- IRL_predicted
st_geometry(IRL_test) <- IRL_full$geometry
IRL_test <- IRL_test %>%
  select(all_of(c("X", "Y", "predicted")))  # automatically selects geometry

plot_IRL <- st_transform(IRL_test, crs = 3857) 
IRL_bb <- st_bbox(c(xmin = -9008088-15000,
                    ymin = 3315585-15000,
                    xmax = -8976838+15000,
                    ymax = 3385010+15000), crs = 3857)
IRL_coords <- as.data.frame(st_coordinates(plot_IRL))
plot_IRL2 <- cbind(plot_IRL[,-c(1,2)], IRL_coords)

# Indian River Lagoon predictions map
IRL_map <- basemaps::basemap_ggplot(ext = IRL_bb) +
  geom_sf(data = plot_IRL2, aes(x = X, y = Y, color = predicted), size = 1) +
  labs(x = "Latitude", y = "Longitude", title = "Indian River Lagoon") +
  scale_color_gradient2(name = "Predicted", midpoint = 0.08,
                        low = "#B31B1B", mid = "#FCF75E", high = "#0BDA51") +
  theme(plot.background = element_blank(),
        panel.background = element_blank(),
        panel.grid = element_blank(),
        legend.background = element_blank())

# Maps
choc_map
pens_map
tampa_map
IRL_map

```

![](C:/Users/erika/OneDrive - University of Central Florida/R/SCRG-additions/SCRG/QGIS/choc_predicted_map.png)

![](C:/Users/erika/OneDrive - University of Central Florida/R/SCRG-additions/SCRG/QGIS/pens_predicted_map.png)

![](C:/Users/erika/OneDrive - University of Central Florida/R/SCRG-additions/SCRG/QGIS/tampa_predicted_map.png)

![](C:/Users/erika/OneDrive - University of Central Florida/R/SCRG-additions/SCRG/QGIS/IRL_predicted_map.png)





